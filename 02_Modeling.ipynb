{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUScxh3a3QckYgJvdU1ef8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theouterlimitz/SDSS_Star_Classification/blob/main/02_Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5qe8GBqDIOy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the clean, prepared dataset\n",
        "df = pd.read_pickle('cleaned_sdss_data.pkl')\n",
        "\n",
        "print(\"Cleaned dataset loaded successfully!\")\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In your 02_Modeling.ipynb notebook, after loading cleaned_sdss_data.pkl\n",
        "\n",
        "print(\"Performing feature engineering: Creating color features...\")\n",
        "df['u-g'] = df['u'] - df['g']\n",
        "df['g-r'] = df['g'] - df['r']\n",
        "df['r-i'] = df['r'] - df['i']\n",
        "df['i-z'] = df['i'] - df['z']\n",
        "\n",
        "# Now our DataFrame 'df' has 4 new, potentially powerful features.\n",
        "# We would then proceed with the train-test split and scaling on this new set of features.\n",
        "\n",
        "print(\"New color features created successfully.\")\n",
        "print(\"Updated columns:\", df.columns.tolist())"
      ],
      "metadata": {
        "id": "Tofvf95Xa5Re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prepare Data for Machine Learning**"
      ],
      "metadata": {
        "id": "TmfJXInKEL5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "print(\"--- Preparing Data for Machine Learning ---\")\n",
        "\n",
        "# 1. Separate features (X) and target (y)\n",
        "X = df.drop(columns=['class']) # All columns except our target\n",
        "y = df['class']               # Just the target column\n",
        "\n",
        "# 2. Encode the target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "print(\"Target labels encoded successfully.\")\n",
        "print(f\"Class mapping: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}\")\n",
        "\n",
        "# 3. Split the data into training and testing sets (stratified)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        ")\n",
        "print(f\"\\nData split into training and testing sets.\")\n",
        "print(f\"Shape of X_train: {X_train.shape}\")\n",
        "print(f\"Shape of X_test: {X_test.shape}\")\n",
        "\n",
        "# 4. Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "print(\"\\nFeatures scaled successfully.\")"
      ],
      "metadata": {
        "id": "YYusK_-GEQ7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train and Evaluate Baseline Model (Random Forest)**"
      ],
      "metadata": {
        "id": "eq_KiwUsElEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"--- Training Random Forest Classifier ---\")\n",
        "\n",
        "# 1. Initialize and Train the Model\n",
        "rfc = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "rfc.fit(X_train_scaled, y_train)\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# 2. Make Predictions and Evaluate\n",
        "print(\"\\n--- Evaluating Model Performance ---\")\n",
        "y_pred = rfc.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy Score: {accuracy:.4f}\")\n",
        "\n",
        "# 3. Display Detailed Reports\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "ConfusionMatrixDisplay.from_estimator(rfc, X_test_scaled, y_test,\n",
        "                                      display_labels=label_encoder.classes_,\n",
        "                                      cmap='Blues', ax=ax)\n",
        "plt.title('Confusion Matrix for Random Forest Classifier')\n",
        "plt.show()\n",
        "\n",
        "# 4. Analyze Feature Importance\n",
        "print(\"\\n--- Analyzing Feature Importance ---\")\n",
        "importances = rfc.feature_importances_\n",
        "feature_importance_df = pd.Series(importances, index=X.columns).sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=feature_importance_df.values, y=feature_importance_df.index, palette='mako')\n",
        "plt.title('Feature Importance from Random Forest', fontsize=16)\n",
        "plt.xlabel('Importance Score')\n",
        "plt.ylabel('Features')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Doua7y7PEwwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building the Neural Network**"
      ],
      "metadata": {
        "id": "PnRikhlxHuyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- Assume these variables are loaded in your notebook's memory ---\n",
        "# X_train_scaled, X_test_scaled, y_train, y_test\n",
        "# label_encoder (for decoding labels later)\n",
        "\n",
        "# ===================================================================\n",
        "# --- Step 1: Build and Train the Neural Network ---\n",
        "# ===================================================================\n",
        "print(\"--- Building the Neural Network ---\")\n",
        "\n",
        "# Define the Model Architecture\n",
        "model = keras.Sequential([\n",
        "    # Input layer: The shape must match the number of features (9)\n",
        "    keras.layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    keras.layers.Dropout(0.3),  # Dropout helps prevent overfitting\n",
        "\n",
        "    # Hidden layer 1\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dropout(0.3),\n",
        "\n",
        "    # Hidden layer 2\n",
        "    keras.layers.Dense(32, activation='relu'),\n",
        "\n",
        "    # Output layer: It must have 3 neurons (one for each class)\n",
        "    # 'softmax' is used for multi-class classification to output a probability for each class.\n",
        "    keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the Model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"Model built and compiled successfully.\")\n",
        "model.summary()\n",
        "\n",
        "# Train the Model\n",
        "print(\"\\n--- Training the Neural Network (this will take a few minutes) ---\")\n",
        "history = model.fit(\n",
        "    X_train_scaled,\n",
        "    y_train,\n",
        "    epochs=20,          # We'll do 20 passes through the training data\n",
        "    batch_size=32,      # Process data in batches of 32\n",
        "    validation_split=0.2, # Use 20% of training data for validation during training\n",
        "    verbose=1           # Show the progress bar\n",
        ")\n",
        "\n",
        "# ===================================================================\n",
        "# --- Step 2: Visualize Training History ---\n",
        "# ===================================================================\n",
        "print(\"\\n--- Visualizing Training History ---\")\n",
        "history_df = pd.DataFrame(history.history)\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_df['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history_df['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy over Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='lower right')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_df['loss'], label='Train Loss')\n",
        "plt.plot(history_df['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss over Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='upper right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ===================================================================\n",
        "# --- Step 3: Evaluate Final Performance on the Unseen Test Set ---\n",
        "# ===================================================================\n",
        "print(\"\\n--- Evaluating Final Model Performance on Test Data ---\")\n",
        "\n",
        "# Evaluate the model on the test set to get final loss and accuracy\n",
        "loss, accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "print(f\"Final Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# To generate a classification report and confusion matrix, we need predictions\n",
        "y_pred_probs = model.predict(X_test_scaled)\n",
        "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nClassification Report (Neural Network):\")\n",
        "print(classification_report(y_test, y_pred_classes, target_names=label_encoder.classes_))\n",
        "\n",
        "# Confusion Matrix\n",
        "print(\"\\nConfusion Matrix (Neural Network):\")\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_classes,\n",
        "                                        display_labels=label_encoder.classes_,\n",
        "                                        cmap='cividis',\n",
        "                                        ax=ax)\n",
        "plt.title('Confusion Matrix for Neural Network')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XOqDVgnrHxhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advanced Network**"
      ],
      "metadata": {
        "id": "2SxI0DXIR1aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
        "from sklearn.utils import class_weight # Make sure to import this\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- Assume these variables are loaded in your notebook's memory ---\n",
        "# X_train_scaled, X_test_scaled, y_train, y_test\n",
        "# label_encoder\n",
        "\n",
        "# ===================================================================\n",
        "# --- Step 7: Build, Train, and Evaluate an Advanced Neural Network ---\n",
        "# ===================================================================\n",
        "print(\"--- Building Advanced Neural Network (V2 with Class Weights) ---\")\n",
        "\n",
        "# 1. Define the Model Architecture (same as before)\n",
        "model_v2_weighted = keras.Sequential([\n",
        "    keras.layers.Dense(256, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    keras.layers.Dropout(0.4),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.Dense(32, activation='relu'),\n",
        "    keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "# 2. Compile the Model (same as before)\n",
        "model_v2_weighted.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"Model V2 built and compiled successfully.\")\n",
        "model_v2_weighted.summary()\n",
        "\n",
        "\n",
        "# --- ** NEW CODE GOES HERE ** ---\n",
        "# 3. Calculate Class Weights to handle imbalance\n",
        "# This must be done before we train the model.\n",
        "print(\"\\nCalculating class weights to handle data imbalance...\")\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "print(\"Calculated Class Weights:\", class_weight_dict)\n",
        "# --- ** END OF NEW CODE ** ---\n",
        "\n",
        "\n",
        "# 4. Train the New Model (with the added class_weight parameter)\n",
        "print(\"\\n--- Training Advanced Neural Network with Class Weights ---\")\n",
        "history_v2_weighted = model_v2_weighted.fit(\n",
        "    X_train_scaled,\n",
        "    y_train,\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    class_weight=class_weight_dict, # <-- ADD THIS PARAMETER\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# --- Evaluate the Weighted Model ---\n",
        "# ===================================================================\n",
        "print(\"\\n--- Evaluating Final Weighted Model Performance on Test Data ---\")\n",
        "\n",
        "loss_v2_w, accuracy_v2_w = model_v2_weighted.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "print(f\"Final Test Accuracy (Weighted Model V2): {accuracy_v2_w:.4f}\")\n",
        "\n",
        "y_pred_probs_v2_w = model_v2_weighted.predict(X_test_scaled)\n",
        "y_pred_classes_v2_w = np.argmax(y_pred_probs_v2_w, axis=1)\n",
        "\n",
        "print(\"\\nClassification Report (Weighted Neural Network V2):\")\n",
        "print(classification_report(y_test, y_pred_classes_v2_w, target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "id": "LR-BV4ygbVeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ensemble**"
      ],
      "metadata": {
        "id": "N467m0wSh-hM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# --- Assume these are trained and available in your notebook's memory ---\n",
        "# rfc: Your trained RandomForestClassifier\n",
        "# model_v2_weighted: Your trained and weighted Keras Neural Network\n",
        "# X_test_scaled, y_test, label_encoder\n",
        "\n",
        "# =======================================================================\n",
        "# --- Final Experiment: Manually Create and Evaluate an Ensemble Model ---\n",
        "# =======================================================================\n",
        "print(\"--- Building Manual Ensemble (Soft Voting) ---\")\n",
        "\n",
        "# 1. Get the predicted probabilities from both models\n",
        "print(\"Getting predictions from Random Forest...\")\n",
        "y_pred_proba_rf = rfc.predict_proba(X_test_scaled)\n",
        "\n",
        "print(\"Getting predictions from Neural Network...\")\n",
        "y_pred_proba_nn = model_v2_weighted.predict(X_test_scaled)\n",
        "\n",
        "# 2. Average the probabilities from both models\n",
        "# This is the core of \"soft voting\"\n",
        "print(\"Averaging model probabilities...\")\n",
        "y_pred_proba_ensemble = (y_pred_proba_rf + y_pred_proba_nn) / 2.0\n",
        "\n",
        "# 3. Determine the final class prediction by finding the class with the highest average probability\n",
        "y_pred_ensemble = np.argmax(y_pred_proba_ensemble, axis=1)\n",
        "print(\"Final predictions calculated.\")\n",
        "\n",
        "\n",
        "# 4. Evaluate the Manual Ensemble Model\n",
        "print(\"\\n--- Evaluating Manual Ensemble Model Performance ---\")\n",
        "accuracy_ensemble = accuracy_score(y_test, y_pred_ensemble)\n",
        "print(f\"Final Test Accuracy (Manual Ensemble): {accuracy_ensemble:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report (Manual Ensemble):\")\n",
        "print(classification_report(y_test, y_pred_ensemble, target_names=label_encoder.classes_))\n",
        "\n",
        "# 5. Generate the Final Confusion Matrix\n",
        "print(\"\\nConfusion Matrix (Manual Ensemble):\")\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_ensemble,\n",
        "                                        display_labels=label_encoder.classes_,\n",
        "                                        cmap='plasma',\n",
        "                                        ax=ax)\n",
        "plt.title('Confusion Matrix for Manual Ensemble Model')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "abFNq11ujAPr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}